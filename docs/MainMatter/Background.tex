\chapter{Marco Te\'orico - Conceptual}\label{chapter:state-of-the-art}

%En el contexto actual de la era digital, los sistemas de recuperación y clasificación de información representan un componente esencial en la gestión y el procesamiento de datos. Estos sistemas, que han experimentado una evolución considerable a lo largo de las últimas décadas, son fundamentales para manejar de manera eficiente y efectiva la creciente avalancha de información digital. Como destacan Manning, Raghavan y Schütze en su obra "Introduction to Information Retrieval" (2008), estos sistemas no solo permiten un acceso rápido y preciso a la información relevante, sino que también juegan un rol crucial en la organización y estructuración de enormes conjuntos de datos. Esta importancia se manifiesta en diversos ámbitos, abarcando desde la bibliotecología y la informática hasta el mundo empresarial, donde la capacidad para recuperar y clasificar información de manera eficaz se ha convertido en una herramienta vital para la toma de decisiones y el desarrollo de estrategias informadas.
%
%La evolución de estos sistemas ha estado inextricablemente ligada al avance tecnológico, adaptándose y evolucionando para afrontar los retos que presenta la gestión de datos en un entorno digital cada vez más complejo. Baeza-Yates y Ribeiro-Neto, en su libro "Modern Information Retrieval" (2011), enfatizan cómo la transformación de estos sistemas refleja un esfuerzo constante por mejorar no solo la precisión y la eficiencia en la recuperación de la información, sino también por aumentar la accesibilidad y la usabilidad para los usuarios finales. Este desafío implica no solo lidiar con la cantidad masiva de información, sino también con la diversidad de formatos, la rapidez con la que se generan nuevos datos y la necesidad de filtrar información relevante de la irrelevante.
%
%Además, estos sistemas de recuperación y clasificación han sido objeto de un interés creciente en el ámbito académico y comercial debido a su capacidad para influir en la eficiencia operativa y en la generación de conocimiento. Conforme señala Liu en "Information Retrieval and Mining in Distributed Environments" (2011), la integración de técnicas avanzadas de procesamiento de datos y algoritmos de aprendizaje automático ha llevado a un aumento notable en la capacidad de estos sistemas para proporcionar resultados relevantes y personalizados. Este avance ha sido crucial para abordar problemas como la sobrecarga de información y la necesidad de filtrar contenido en un mundo cada vez más saturado de datos.
%
%En este marco, el desarrollo y la mejora continua de los sistemas de recuperación y clasificación de información se perfilan como un área de investigación y desarrollo de gran relevancia. Esta relevancia no solo se evidencia en la mejora de las capacidades técnicas de estos sistemas, sino también en su capacidad para adaptarse a las necesidades cambiantes de usuarios y organizaciones en un entorno digital globalizado.
%
%Además de la evolución general de los sistemas de recuperación y clasificación de información, es importante destacar las diversas vertientes y especializaciones que han surgido dentro de este campo. Una de estas vertientes es la recuperación de información textual, que se centra en el procesamiento y recuperación de documentos escritos. Baeza-Yates y Ribeiro-Neto (2011) en "Modern Information Retrieval" enfatizan cómo esta área aborda desafíos únicos asociados con el procesamiento del lenguaje natural y la comprensión semántica del texto. Por otro lado, la recuperación de información multimedia, como discuten Lew et al. en "Content-Based Multimedia Information Retrieval: State of the Art and Challenges" (2006), se dedica a la búsqueda y clasificación de contenido que incluye imágenes, audio y video, enfrentando desafíos específicos como el análisis de contenido visual y la extracción de características audiovisuales.
%
%Otra área importante es la recuperación de información geográfica, donde se combinan datos espaciales y convencionales para ofrecer resultados basados en la ubicación. Este enfoque, explorado por Purves et al. en "Geographic Information Retrieval: Progress and Challenges in Spatial Search of Text" (2018), es vital en aplicaciones como los sistemas de información geográfica (SIG) y los servicios basados en la localización. Además, la recuperación de información en la web, que se ocupa de la búsqueda y clasificación de información en Internet, ha ganado prominencia, como se detalla en "Search Engines: Information Retrieval in Practice" de Croft, Metzler y Strohman (2009). Esta vertiente aborda desafíos únicos como el manejo de la dinámica del contenido web, el ranking de páginas y la optimización para motores de búsqueda.
%
%Por último, en el ámbito académico y científico, la recuperación de información bibliográfica, como se describe en "The Elements of Library Research: What Every Student Needs to Know" de George (2008), se enfoca en la búsqueda y organización de literatura académica y científica. Esta especialización es fundamental para la investigación académica, ya que facilita el acceso a publicaciones, artículos de revistas y otros materiales académicos relevantes.
%
%Dentro de la vertiente de sistemas de recuperación de información textual, el Procesamiento del Lenguaje Natural (NLP, por sus siglas en inglés) emerge como un componente crítico. El NLP es un campo de la inteligencia artificial que se enfoca en la interacción entre las computadoras y el lenguaje humano, buscando dotar a las máquinas de la capacidad para entender, interpretar y manipular el lenguaje natural. Jurafsky y Martin, en su libro "Speech and Language Processing" (2019), destacan que el NLP combina la informática, la inteligencia artificial y la lingüística para llenar el vacío entre la comunicación humana y la comprensión computacional.
%
%La necesidad de incorporar semántica en los sistemas de recuperación de información textual es fundamental en la actualidad debido a varias razones. Primero, como Manning y Schütze argumentan en "Foundations of Statistical Natural Language Processing" (1999), la semántica permite a los sistemas entender el significado contextual y las intenciones detrás de las consultas de los usuarios, mejorando significativamente la precisión y relevancia de los resultados de búsqueda. Esto es especialmente importante en la era del big data, donde la cantidad abrumadora de información disponible hace que la precisión y la relevancia sean cruciales para la eficiencia de la recuperación de información.
%
%Además, la incorporación de semántica a través del NLP ayuda a superar los desafíos asociados con la ambigüedad del lenguaje, como los sinónimos, los homónimos y las variaciones idiomáticas. En "Natural Language Processing with Python" (2009), Bird, Klein y Loper explican cómo el NLP puede analizar el lenguaje en varios niveles (desde el léxico hasta el pragmático) para comprender mejor el significado real de las palabras y frases en su contexto.
%
%Finalmente, en un mundo cada vez más globalizado, donde el acceso a información diversa es esencial, el NLP habilita la recuperación de información en múltiples idiomas, lo que permite a los usuarios acceder a un espectro más amplio de conocimientos y perspectivas. Esto resalta la importancia de desarrollar sistemas de recuperación de información textual más sofisticados y semánticamente ricos, capaces de manejar la complejidad y la riqueza del lenguaje humano en todas sus formas.

En la esfera contemporánea de la gestión de la información, los sistemas de recuperación de información desempeñan un papel fundamental. Estos sistemas, que han evolucionado significativamente a lo largo de las últimas décadas, son cruciales para el manejo eficiente de la creciente marea de datos digitales. Según Manning, Raghavan y Schütze (2008) en ``Introduction to Information Retrieval'', estos sistemas no solo facilitan el acceso rápido a información relevante sino que también contribuyen a la organización y estructuración de grandes volúmenes de datos. Esta relevancia se extiende a través de una variedad de campos, desde la bibliotecología y la informática hasta los negocios, donde la capacidad de recuperar y clasificar información de manera efectiva se ha convertido en una herramienta indispensable para la toma de decisiones y el conocimiento estratégico.

Los sistemas de recuperación de información se han diversificado en varias vertientes especializadas, reflejando la complejidad y diversidad de las necesidades en este campo. Un componente esencial en la recuperación de información textual es el Procesamiento del Lenguaje Natural (NLP). Como señalan Jurafsky y Martin en ``Speech and Language Processing'' (2019), el NLP permite a las máquinas comprender, interpretar y manipular el lenguaje humano, proporcionando una comprensión profunda del significado y el contexto, crucial para superar desafíos como la ambigüedad y la variedad idiomática. Esta habilidad no solo mejora la interpretación del lenguaje a múltiples niveles, sino que también juega un papel clave en la organización y clasificación de grandes volúmenes de datos textuales. El NLP ha revolucionado la búsqueda y recuperación de información, permitiendo análisis y síntesis más precisos y rápidos, esenciales en campos como la investigación académica. Además, ha mejorado la accesibilidad, facilitando interfaces de usuario más naturales e intuitivas, como en sistemas de preguntas y respuestas y asistentes virtuales.

En cuanto a la recuperación web y bibliográfica, como describen Croft, Metzler y Strohman en ``Search Engines: Information Retrieval in Practice'' (2009) y George en ``The Elements of Library Research'' (2008) respectivamente, se enfocan en desafíos especializados dentro del campo. La recuperación web aborda aspectos como el ranking de páginas y la optimización para motores de búsqueda, mientras que la recuperación bibliográfica se centra en la organización y el acceso a la literatura académica y científica. 

Por otro lado, la recuperación de información multimedia, como abordan Lew et al. en ``Content-Based Multimedia Information Retrieval'' (2006), se ocupa de contenido que incluye imágenes, audio y video. También está la recuperación de información geográfica, crucial en aplicaciones como los sistemas de información geográfica (SIG) y los servicios basados en la localización, como se describe en ``Geographic Information Retrieval'' de Purves et al. (2018).

En el presente capítulo, se discuten brevemente componentes clave dentro del campo de la recuperación textual que constituyen la base te\'orica de esta investigaci\'on: los modelos de tópicos, ontologías y \textit{embeddings}, así como los antecedentes relevantes para la automatización de la estimación del número de tópicos en un corpus y la asignación de nombres a los mismos.

\section{Modelos de T\'opicos}

Los modelos de tópicos, fundamentales en NLP y la recuperación de información, se destacan por su capacidad para explorar y organizar grandes conjuntos de datos textuales. Estos modelos operan identificando tópicos subyacentes en colecciones de documentos, extrayendo patrones significativos en el uso de palabras y revelando así las estructuras temáticas latentes. Esta funcionalidad los convierte en herramientas esenciales para comprender, categorizar y sintetizar información en grandes volúmenes de texto, lo que es crucial en una era donde la cantidad de datos disponibles crece exponencialmente.

El desarrollo histórico de los modelos de tópicos es un viaje fascinante a través de la evolución del NLP. Comenzando con el Análisis Semántico Latente (LSA) en 1990 por Deerwester et al. (``Indexing by latent semantic analysis''), un método pionero que utilizaba la descomposición en valores singulares para identificar estructuras semánticas en grandes colecciones de texto. Esta técnica fue fundamental para sentar las bases de los modelos de tópicos.
El Análisis Semántico Latente Probabilístico (pLSA), propuesto por Hofmann en 1999, representó un avance significativo, introduciendo un enfoque probabilístico para modelar la relación entre documentos y tópicos. Sin embargo, pLSA tenía limitaciones, especialmente en la generalización a documentos no vistos durante el entrenamiento.

El gran avance llegó con la introducción de la Asignación Latente de Dirichlet (LDA) por Blei et al. en 2003 (``Latent Dirichlet Allocation''), considerando cada documento como una mezcla de tópicos latentes, donde cada tópico está definido por una distribución sobre las palabras. Formalmente,
para cada documento \( d \), LDA asume una distribución de tópicos \( \theta_d \) que se extrae de una distribución a priori de Dirichlet. Para cada palabra en el documento, se elige un tópico \( z \) de la distribución de tópicos \( \theta_d \). Luego, se selecciona una palabra de una distribución de palabras asociada a ese tópico específico.
Este proceso se repite a lo largo de todos los documentos y palabras, iterando para ajustar las distribuciones de tópicos y palabras hasta que el modelo refleje adecuadamente la estructura latente de tópicos en los documentos. Este modelo generativo probabilístico ofreció una mayor flexibilidad y capacidad de interpretación, convirtiéndose en el estándar de oro para el modelado de tópicos.

Desde entonces, los modelos de tópicos han continuado evolucionando, integrando enfoques más sofisticados como el Modelo de Tópicos Correlacionados (CTM) y el Modelo de Tópicos Dinámicos (DTM). El CTM, al incorporar correlaciones entre tópicos, ofrece una representación más matizada y realista de cómo se distribuyen los tópicos en documentos, mientras que el DTM introduce una perspectiva temporal, analizando cómo los tópicos evolucionan con el tiempo. Además, los modelos jerárquicos como el Hierarchical Latent Dirichlet Allocation (HLDA) han proporcionado una estructura más compleja, permitiendo la detección de tópicos en distintos niveles de granularidad. Estos avances han enriquecido el análisis de tópicos, ofreciendo una visión más profunda y detallada de las estructuras temáticas en grandes volúmenes de texto.

%Los modelos de tópicos se aplican en una diversidad de campos, demostrando su versatilidad y utilidad. En plataformas de streaming y noticias, facilitan la recomendación de contenido relevante basado en los intereses del usuario. En redes sociales, son cruciales para analizar tendencias y discusiones. En la gestión de bibliotecas digitales, ayudan a organizar y buscar información temática. Son herramientas valiosas en la investigación académica para revisar y clasificar literatura científica. En marketing, identifican patrones de comportamiento y preferencias de los consumidores. En el sector financiero, se utilizan para análisis de sentimientos y predicción de tendencias de mercado. En salud pública, permiten analizar discursos y tendencias sobre temas de salud. En el ámbito gubernamental, facilitan la gestión y clasificación de documentos oficiales. En educación, son útiles para clasificar y recomendar material educativo, y en inteligencia de negocios, permiten el análisis de grandes volúmenes de datos y reportes.

%Sin embargo, este campo no est\'a excento de retos. Determinar el número adecuado de tópicos es uno de los desafíos más significativos y tiene un impacto directo en la calidad y utilidad de los modelos ya que un número demasiado bajo puede fusionar tópicos distintos, mientras que un número excesivo puede resultar en tópicos superpuestos o irrelevantes, afectando la claridad y utilidad del análisis. La interpretación de los tópicos generados también es un desafío crítico, ya que la subjetividad y variabilidad en la interpretación humana pueden llevar a interpretaciones inconsistentes y dificultar la comparación entre estudios. Además, la gestión del ruido en los datos es esencial para asegurar que los tópicos reflejen temas genuinos. Estos desafíos resaltan la necesidad de herramientas y métodos más robustos para mejorar la interpretación y eficacia de los modelos de tópicos. 

\section{Ontolog\'ias}

Las ontologías, en el contexto de la informática y NLP, son estructuras de datos que representan conocimientos de manera organizada y jerárquica. Según Guarino (1998) en ``Formal Ontologies and Information Systems'', una ontología define un conjunto de conceptos y categorías que representan un dominio de conocimiento, así como las relaciones entre estos conceptos. Computacionalmente, las ontologías se manejan frecuentemente como grafos, donde los nodos representan conceptos o entidades, y las aristas o bordes representan las relaciones entre ellos. Las ontologías permiten que las máquinas ``comprendan'' y procesen el significado de la información de manera más eficaz, fundamental para la extracción y clasificación de información, donde se requiere no solo identificar datos, sino también comprender su sem\'antica.

Según Uschold y Grüninger (1996), las ontologías son creadas principalmente por expertos en un dominio específico, con el objetivo de facilitar la comunicación y la comprensión común en diferentes campos. Estos expertos utilizan las ontologías para establecer un marco conceptual compartido, lo que ayuda a superar las barreras de comunicación y a mejorar la interoperabilidad entre sistemas y organizaciones, lo cual es fundamental en áreas como la ingeniería de sistemas, la integración empresarial y el desarrollo de software.

Las ontologías tienen aplicaciones diversas en la informática y el NLP. Berners-Lee et al. (2001) resaltan su uso en la Web Semántica, al mejorar la accesibilidad y gestión de información, facilitando búsquedas más eficientes y una navegación intuitiva. Hotho et al. (2002) y Lastra-Díaz et al. (2019) exploran su uso en la agrupación de documentos y la similitud semántica. Estas técnicas mejoran la precisión en la categorización y búsqueda de documentos, permitiendo a los sistemas informáticos identificar conexiones y similitudes en el contenido textual basándose en significados subyacentes. Batet et al. (2009) y Corcho (2006) se centran en la clasificación y anotación de documentos, demostrando cómo las ontologías enriquecen la recuperación y gestión de información. Su integración proporciona una mayor profundidad en el análisis de textos, permitiendo la creación de metadatos más ricos y relevantes, lo que mejora sustancialmente la recuperación y gestión de información en bases de datos y repositorios digitales.

Pueden ser clasificadas como ontologías de dominio específico y ontologías generales. Las ontologías de dominio específico se enfocan en áreas de conocimiento particulares, brindando un marco detallado para las entidades y relaciones dentro de ese ámbito específico. Por ejemplo, en el campo de la medicina, la ``Ontología de la Genómica del Cáncer'' (OGC) es una ontología de dominio específico que detalla la terminología, las relaciones y los procesos relacionados con el cáncer y la genómica. Este tipo de ontología es indispensable en aplicaciones médicas y de investigación, proporcionando una estructura precisa para la gestión y el análisis de datos complejos relacionados con enfermedades y tratamientos.

En contraste, las ontologías generales abarcan un conocimiento más amplio, estableciendo un marco general para clasificar y relacionar conceptos de varios dominios. Son esenciales en aplicaciones que demandan un entendimiento generalizado del conocimiento humano. WordNet es un ejemplo significativo en esta categoría; es una base de datos léxica en inglés que organiza palabras en conjuntos de sinónimos (synsets), definiendo relaciones como sinonimia, antonimia y jerarquías de hipónimos (especificaciones) e hiperónimos (generalizaciones).

%La construcción de ontologías es un proceso complejo que involucra varias etapas. Inicialmente, se realiza la identificación y definición de conceptos clave y sus interrelaciones en un dominio específico. Este proceso es generalmente llevado a cabo por expertos en el dominio en colaboración con ingenieros. Luego, se desarrolla una taxonomía que organiza estos conceptos de manera jerárquica. Además, se especifican propiedades y restricciones para estos conceptos y relaciones. Las ontologías se construyen utilizando lenguajes formales como OWL, permitiendo su uso en sistemas informáticos para mejorar la búsqueda y recuperación de información, facilitar la interoperabilidad entre sistemas, y proporcionar una base sólida para el desarrollo de aplicaciones de inteligencia artificial y la Web Semántica 

\section{Vectores con contenido sem\'antico: \textit{embeddings}}

Los \textit{embeddings}, en el contexto de NLP, son representaciones vectoriales de palabras o frases. Almeida y Xexéo (2023) en su estudio ``Word Embeddings: A Survey'', explican que estos \textit{embeddings} son vectores densos y distribuidos de longitud fija, construidos utilizando estadísticas de co-ocurrencia de palabras. Estas representaciones codifican información sintáctica y semántica, transformando el texto en una forma que es manejable para los algoritmos de aprendizaje automático, facilitando tareas como clasificación de texto, análisis de sentimientos y traducción automática.

Existen varios tipos de \textit{embeddings}, cada uno con características únicas. Los \textit{word embeddings}, con modelos preentrenados como Word2Vec y GloVe, representan palabras individuales como vectores en un espacio multidimensional, capturando su significado y relaciones sintácticas. Los \textit{sentence embeddings}, por otro lado, representan oraciones enteras, permitiendo capturar el contexto más amplio de la oración. Los \textit{contextual embeddings} como los generados por modelos como BERT, van un paso más allá, representando palabras en el contexto de frases o párrafos, lo que permite una comprensión más matizada del significado, especialmente para palabras con múltiples interpretaciones.

Existen distintas investigaciones que ilustran la versatilidad de los \textit{embeddings} en diversas áreas para el análisis de textos y relaciones semánticas.. El estudio de Fang et al. (2016) demuestra la aplicación de \textit{embeddings} en la evaluación de la coherencia de temas en datos de Twitter. Utilizando \textit{embeddings}, se pueden medir con precisión la coherencia y relevancia de los tópicos generados, lo que es crucial para la interpretación efectiva de grandes conjuntos de datos sociales. Además, Kuzi et al. (2016) exploran cómo los \textit{embeddings} pueden mejorar la expansión de consultas en motores de búsqueda, proporcionando una búsqueda más rica y contextualizada. Otros ejemplos incluyen el trabajo de Kusner et al., que examina cómo los \textit{embeddings} de palabras se utilizan para medir distancias entre documentos, y Lezama-Sánchez et al. (2022), que investiga el uso de \textit{embeddings} basados en relaciones semánticas para mejorar el análisis de texto. Además, Liu et al. exploran los \textit{embeddings} temáticos, mostrando su aplicación en la comprensión de tópicos específicos en grandes conjuntos de datos. 

\section{Estado del arte}

La sección del estado del arte explora desarrollos recientes y metodologías clave en los dos problemas centrales a abordar en este trabajo: la automatizaci\'on de la identificación del número de tópicos en un corpus y de la asignaci\'on de nombres a t\'opicos. 

\subsection{Identificación del n\'umero de t\'opicos presentes en un corpus}

La identificación del número de tópicos en un corpus es crucial en el análisis de datos y el NLP, ya que define la estructura y claridad en la interpretación de grandes volúmenes de texto. Hasta ahora, este proceso depende en gran medida de la intervención de expertos, lo que conlleva una subjetividad inherente y limita la escalabilidad y consistencia en el análisis. La ausencia de métodos automáticos para determinar el número óptimo de tópicos representa un desafío significativo, ya que un enfoque automatizado y objetivo podría adaptarse mejor a la naturaleza dinámica y variable de los datos, permitiendo a los analistas centrarse más en la interpretación y aplicación de los resultados.

El enfoque de Arun et al. (2010) en ``On Finding the Natural Number of Topics with Latent Dirichlet Allocation'' se centra en determinar el número óptimo de tópicos para modelos LDA. Proponen una medida basada en la divergencia simétrica de Kullback-Leibler de las distribuciones salientes de los factores de la matriz LDA. Esta medida identifica el número de tópicos observando un 'pico' en los valores de divergencia para el número correcto de tópicos. El método es validado con conjuntos de datos reales y sintéticos.

Por otro lado, el art\'iculo de Gan y Qi (2021) en ``Selection of the Optimal Number of Topics for LDA'' se centra en una metodología integral para determinar el número óptimo de tópicos en LDA. Presentan un índice que evalúa varios factores: perplejidad, aislamiento de tópicos, estabilidad y coincidencia. Este índice tiene como objetivo lograr una alta capacidad predictiva, un buen aislamiento entre tópicos, evitar tópicos duplicados y asegurar la repetibilidad. Se validó con datasets generales y una aplicación específica en la clasificación de políticas de patentes en China, mostrando buenos resultados.

Vangara et al. (2021) en ``Finding the Number of Latent Topics With Semantic Non-Negative Matrix Factorization'' introduce SeNMFk, una metodología que combina la factorización de matrices no negativas (NMF) con información semántica para determinar el número óptimo de tópicos. SeNMFk utiliza la divergencia de Kullback-Leibler y se enfoca en la estabilidad de los tópicos a través de un ensamble aleatorio de matrices. Además, presentan el software pyDNMFk para facilitar la estimación del número de tópicos.

Los siguientes art\'iculos, aunque no persiguen el objetivo de identificar autom\'aticamente el n\'umero de t\'opicos presentes en el corpus, son relevantes ya que emplean razonamientos b\'asicos, an\'alogos a los utilizados en esta investigaci\'on. El enfoque de Jiang et al. (2011) en ``A Fuzzy Self-Constructing Feature Clustering Algorithm for Text Classification'' se basa en un algoritmo de agrupación de características autoconstruido difuso. Este método se enfoca en identificar estructuras latentes en datos de texto, utilizando técnicas de agrupación difusa para manejar la incertidumbre y la ambigüedad inherentes a los datos textuales. Permite una clasificación más flexible y adaptativa de los datos, lo que puede ser especialmente útil en aplicaciones donde las categorías no son claramente definidas o donde los datos pueden pertenecer a múltiples categorías simultáneamente, sin tener que especificar el n\'umero de grupos como par\'ametro de entrada. Por otro lado, el enfoque de Thompson y Mimno (2020) en ``Topic Modeling with Contextualized Word Representation Clusters'' investiga el uso de agrupaciones de representaciones de palabras contextualizadas, como las de BERT y GPT-2, para el modelado de tópicos. Su metodología se basa en la hipótesis de que estas representaciones contextualizadas pueden capturar polisemia y proporcionar información sintáctica más rica, lo que resulta en una organización de documentos similar a la obtenida con modelos LDA tradicionales.

\subsection{Asignaci\'on de nombres a grupos}

La asignación automática de nombres a tópicos en un corpus es de gran importancia pues facilita la comprensión e interpretación de los resultados generados por modelos de tópicos. Actualmente, esta tarea tambi\'en se realiza principalmente a través de la intervención de expertos, un proceso que puede ser subjetivo y laborioso. La carencia de métodos automáticos para esta asignación representa un desafío significativo en la eficiencia y objetividad del análisis de grandes conjuntos de datos. La automatización de este proceso permitiría una identificación de tópicos más coherente y objetiva, reduciendo la carga de trabajo manual y aumentando la capacidad para manejar abundante informaci\'on.

El documento ``An Ontology-based Semantic Tagger for IE system'' de Boufaden (2003), detalla un sistema para etiquetar semánticamente conversaciones en el ámbito de búsqueda y rescate marítimo, y facilitar el proceso de la extracci\'on de informaci\'on. La metodología combina dos fuentes de conocimiento: una ontología SAR y el diccionario-tesauro Wordsmyth. El proceso se divide en cuatro etapas: extracción de palabras candidatas, anotación semántica, filtrado contextual, y utilización de las palabras anotadas para la resolución de correferencias y el llenado de plantillas de extracción de información. Se ejemplifica con un diálogo donde se etiquetan palabras clave con etiquetas específicas del dominio, como STATUS MISSING-VESSEL y LOCATION. 

En ``Ontology based Web Page Topic Identification'' de Singh Rathore y Roy se describe un método para identificar t\'opicos en páginas web usando una ontología de dominio espec\'ifico desarrollada manualmente con este prop\'osito. El proceso inicia con la extracción de palabras clave de tags HTML y la detección de co-ocurrencias de palabras en el texto. Primero, se mapean las palabras clave extraídas de la página web a conceptos en la ontología, utilizando la Distancia de Levenshtein para evaluar su similitud. Las palabras clave se clasifican por relevancia, y se establece un umbral para determinar la adecuación del mapeo. En la primera fase, se consideran las palabras clave más relevantes, y si su correspondencia con un nodo de la ontología es significativa, se sugiere que la página pertenece a ese tema. Si no se alcanza este umbral, la segunda fase incluye todas las palabras clave. Si la combinación de todas las palabras clave mapeadas supera el umbral para un nodo, entonces se asigna ese tema al documento. 

El trabajo de Saqlain et al. (2016) sigue un enfoque que utiliza WordNet y TF-IDF para asignar nombres automáticamente a tópicos, combinando técnicas semánticas y estadísticas. El proceso inicia con una agrupación jerárquica de los textos y su posterior procesamiento, incluyendo la eliminación de palabras comunes y la estandarización de términos. Posteriormente, se identifican términos clave en cada grupo utilizando el cálculo de Term Frequency-Inverse Document Frequency (TF-IDF). Estos términos son procesados a través de WordNet para generar sus hiperónimos, y la frecuencia de estos hiperónimos se calcula dentro del grupo. Los hiperónimos que aparecen con mayor frecuencia se seleccionan como etiquetas para los grupos.

\subsubsection{Desambiguaci\'on del sentido de las palabras}

La desambiguación de sentidos de palabras (WSD) es un campo de la lingüística computacional y el NLP que se enfoca en asignar significados precisos a palabras en contextos específicos, diferenciando entre múltiples significados o sentidos. Esto se hace particularmente necesario al utilizar ontolog\'ias ya que una palabra puede corresponder a m\'ultiples conceptos en la ontolog\'ia, cada uno con un significado distinto de acuerdo al contexto.

El algoritmo de Lesk (1986), es una técnica clásica para WSD. El algoritmo opera comparando las definiciones de una palabra objetivo con las palabras presentes en su contexto inmediato. La definici\'on que tenga el mayor solapamiento es elegida como la más probable. A pesar de su simplicidad, el algoritmo de Lesk ha sido fundamental en el desarrollo de técnicas más avanzadas de WSD, y ha sido implementado en disc\'imiles bibliotecas de NLP. Tiene limitaciones evidentes, como su dependencia de coincidencias exactas de palabras y la posibilidad de que el contexto no ofrezca suficiente información para un solapamiento significativo, lo que puede resultar en la elección de sentidos incorrectos para las palabras. Desde entonces han surgido modificaciones de este algoritmo para a\~nadir sem\'antica al proceso. 

Ambos, Edmonds y Agirre (2008), y Bevilacqua et al. (2021), proporcionan un análisis de los algoritmos y aplicaciones en WSD. Los algoritmos basados en grafos son fundamentales en los enfoques basados en el conocimiento, aprovechando estructuras de grafos de recursos léxicos como WordNet y BabelNet. Estos algoritmos ayudan a establecer conexiones entre diferentes sentidos de palabras basándose en sus relaciones semánticas. Por otro lado, en los enfoques supervisados, los modelos neuronales, especialmente aquellos que utilizan arquitecturas de Transformer preentrenadas, han demostrado ser altamente efectivos. Estos modelos aprenden a asociar palabras en contextos específicos con sus sentidos correspondientes, utilizando grandes conjuntos de datos anotados. Además, las técnicas que incorporan glosas o definiciones textuales de inventarios de sentidos, como SensEmBERT y ARES, también han mostrado un rendimiento sobresaliente en WSD. Estos métodos aprovechan las representaciones contextualizadas de palabras y los embeddings de sentido para mejorar la precisión de la desambiguación.

Tambi\'en se destaca el uso de inteligencia artificial y metaheur\'isticas para WSD. El art\'iculo de AL-Saiagh et al. (2018) introduce un enfoque híbrido que combina la optimización de enjambres de partículas (PSO) con el recocido simulado, mientras que ``A Self-adaptive Genetic Algorithm for the Word Sense Disambiguation Problem'' de Wojdan Alsaeedan y Mohamed El Bachir Menai (2015) explora el uso de un algoritmo genético autoadaptativo que ajusta automáticamente las probabilidades de cruce y mutación optimizando el proceso.
