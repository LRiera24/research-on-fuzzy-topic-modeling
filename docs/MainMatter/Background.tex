\chapter{Marco Te\'orico - Conceptual}\label{chapter:state-of-the-art}

%En el contexto actual de la era digital, los sistemas de recuperación y clasificación de información representan un componente esencial en la gestión y el procesamiento de datos. Estos sistemas, que han experimentado una evolución considerable a lo largo de las últimas décadas, son fundamentales para manejar de manera eficiente y efectiva la creciente avalancha de información digital. Como destacan Manning, Raghavan y Schütze en su obra "Introduction to Information Retrieval" (2008), estos sistemas no solo permiten un acceso rápido y preciso a la información relevante, sino que también juegan un rol crucial en la organización y estructuración de enormes conjuntos de datos. Esta importancia se manifiesta en diversos ámbitos, abarcando desde la bibliotecología y la informática hasta el mundo empresarial, donde la capacidad para recuperar y clasificar información de manera eficaz se ha convertido en una herramienta vital para la toma de decisiones y el desarrollo de estrategias informadas.
%
%La evolución de estos sistemas ha estado inextricablemente ligada al avance tecnológico, adaptándose y evolucionando para afrontar los retos que presenta la gestión de datos en un entorno digital cada vez más complejo. Baeza-Yates y Ribeiro-Neto, en su libro "Modern Information Retrieval" (2011), enfatizan cómo la transformación de estos sistemas refleja un esfuerzo constante por mejorar no solo la precisión y la eficiencia en la recuperación de la información, sino también por aumentar la accesibilidad y la usabilidad para los usuarios finales. Este desafío implica no solo lidiar con la cantidad masiva de información, sino también con la diversidad de formatos, la rapidez con la que se generan nuevos datos y la necesidad de filtrar información relevante de la irrelevante.
%
%Además, estos sistemas de recuperación y clasificación han sido objeto de un interés creciente en el ámbito académico y comercial debido a su capacidad para influir en la eficiencia operativa y en la generación de conocimiento. Conforme señala Liu en "Information Retrieval and Mining in Distributed Environments" (2011), la integración de técnicas avanzadas de procesamiento de datos y algoritmos de aprendizaje automático ha llevado a un aumento notable en la capacidad de estos sistemas para proporcionar resultados relevantes y personalizados. Este avance ha sido crucial para abordar problemas como la sobrecarga de información y la necesidad de filtrar contenido en un mundo cada vez más saturado de datos.
%
%En este marco, el desarrollo y la mejora continua de los sistemas de recuperación y clasificación de información se perfilan como un área de investigación y desarrollo de gran relevancia. Esta relevancia no solo se evidencia en la mejora de las capacidades técnicas de estos sistemas, sino también en su capacidad para adaptarse a las necesidades cambiantes de usuarios y organizaciones en un entorno digital globalizado.
%
%Además de la evolución general de los sistemas de recuperación y clasificación de información, es importante destacar las diversas vertientes y especializaciones que han surgido dentro de este campo. Una de estas vertientes es la recuperación de información textual, que se centra en el procesamiento y recuperación de documentos escritos. Baeza-Yates y Ribeiro-Neto (2011) en "Modern Information Retrieval" enfatizan cómo esta área aborda desafíos únicos asociados con el procesamiento del lenguaje natural y la comprensión semántica del texto. Por otro lado, la recuperación de información multimedia, como discuten Lew et al. en "Content-Based Multimedia Information Retrieval: State of the Art and Challenges" (2006), se dedica a la búsqueda y clasificación de contenido que incluye imágenes, audio y video, enfrentando desafíos específicos como el análisis de contenido visual y la extracción de características audiovisuales.
%
%Otra área importante es la recuperación de información geográfica, donde se combinan datos espaciales y convencionales para ofrecer resultados basados en la ubicación. Este enfoque, explorado por Purves et al. en "Geographic Information Retrieval: Progress and Challenges in Spatial Search of Text" (2018), es vital en aplicaciones como los sistemas de información geográfica (SIG) y los servicios basados en la localización. Además, la recuperación de información en la web, que se ocupa de la búsqueda y clasificación de información en Internet, ha ganado prominencia, como se detalla en "Search Engines: Information Retrieval in Practice" de Croft, Metzler y Strohman (2009). Esta vertiente aborda desafíos únicos como el manejo de la dinámica del contenido web, el ranking de páginas y la optimización para motores de búsqueda.
%
%Por último, en el ámbito académico y científico, la recuperación de información bibliográfica, como se describe en "The Elements of Library Research: What Every Student Needs to Know" de George (2008), se enfoca en la búsqueda y organización de literatura académica y científica. Esta especialización es fundamental para la investigación académica, ya que facilita el acceso a publicaciones, artículos de revistas y otros materiales académicos relevantes.
%
%Dentro de la vertiente de sistemas de recuperación de información textual, el Procesamiento del Lenguaje Natural (NLP, por sus siglas en inglés) emerge como un componente crítico. El NLP es un campo de la inteligencia artificial que se enfoca en la interacción entre las computadoras y el lenguaje humano, buscando dotar a las máquinas de la capacidad para entender, interpretar y manipular el lenguaje natural. Jurafsky y Martin, en su libro "Speech and Language Processing" (2019), destacan que el NLP combina la informática, la inteligencia artificial y la lingüística para llenar el vacío entre la comunicación humana y la comprensión computacional.
%
%La necesidad de incorporar semántica en los sistemas de recuperación de información textual es fundamental en la actualidad debido a varias razones. Primero, como Manning y Schütze argumentan en "Foundations of Statistical Natural Language Processing" (1999), la semántica permite a los sistemas entender el significado contextual y las intenciones detrás de las consultas de los usuarios, mejorando significativamente la precisión y relevancia de los resultados de búsqueda. Esto es especialmente importante en la era del big data, donde la cantidad abrumadora de información disponible hace que la precisión y la relevancia sean cruciales para la eficiencia de la recuperación de información.
%
%Además, la incorporación de semántica a través del NLP ayuda a superar los desafíos asociados con la ambigüedad del lenguaje, como los sinónimos, los homónimos y las variaciones idiomáticas. En "Natural Language Processing with Python" (2009), Bird, Klein y Loper explican cómo el NLP puede analizar el lenguaje en varios niveles (desde el léxico hasta el pragmático) para comprender mejor el significado real de las palabras y frases en su contexto.
%
%Finalmente, en un mundo cada vez más globalizado, donde el acceso a información diversa es esencial, el NLP habilita la recuperación de información en múltiples idiomas, lo que permite a los usuarios acceder a un espectro más amplio de conocimientos y perspectivas. Esto resalta la importancia de desarrollar sistemas de recuperación de información textual más sofisticados y semánticamente ricos, capaces de manejar la complejidad y la riqueza del lenguaje humano en todas sus formas.

En la esfera contemporánea de la gestión de la información, los sistemas de recuperación y clasificación de información desempeñan un papel fundamental. Estos sistemas, que han evolucionado significativamente a lo largo de las últimas décadas, son cruciales para el manejo eficiente de la creciente marea de datos digitales. Según Manning, Raghavan y Schütze (2008) en su obra seminal "Introduction to Information Retrieval", estos sistemas no solo facilitan el acceso rápido a información relevante sino que también contribuyen a la organización y estructuración de grandes volúmenes de datos. Esta relevancia se extiende a través de una variedad de campos, desde la bibliotecología y la informática hasta los negocios, donde la capacidad de recuperar y clasificar información de manera efectiva se ha convertido en una herramienta indispensable para la toma de decisiones y el conocimiento estratégico.

La evolución de estos sistemas ha estado inextricablemente ligada al avance tecnológico, adaptándose y evolucionando para afrontar los retos que presenta la gestión de datos en un entorno digital cada vez más complejo. Baeza-Yates y Ribeiro-Neto, en su libro "Modern Information Retrieval" (2011), enfatizan cómo la transformación de estos sistemas refleja un esfuerzo constante por mejorar no solo la precisión y la eficiencia en la recuperación de la información, sino también por aumentar la accesibilidad y la usabilidad para los usuarios finales.

Además, estas plataformas se han diversificado en varias vertientes especializadas. En el ámbito de la recuperación de información textual, el Procesamiento del Lenguaje Natural (NLP) es un componente esencial. El NLP, como se explica en "Speech and Language Processing" de Jurafsky y Martin (2019), permite a las máquinas comprender, interpretar y manipular el lenguaje humano. La incorporación de semántica a través del NLP es crucial para superar desafíos como la ambigüedad y la variedad idiomática, permitiendo una comprensión más profunda del significado y el contexto, como discuten Bird, Klein y Loper en "Natural Language Processing with Python" (2009).

Por otro lado, la recuperación de información multimedia, como abordan Lew et al. en "Content-Based Multimedia Information Retrieval" (2006), se ocupa de contenido que incluye imágenes, audio y video. También está la recuperación de información geográfica, crucial en aplicaciones como los sistemas de información geográfica (SIG) y los servicios basados en la localización, como se describe en "Geographic Information Retrieval" de Purves et al. (2018). Además, la recuperación de información en la web y la bibliográfica, como detallan Croft, Metzler y Strohman en "Search Engines: Information Retrieval in Practice" (2009) y George en "The Elements of Library Research" (2008), respectivamente, abordan desafíos únicos en sus campos.

En conclusión, el desarrollo y la mejora continua de los sistemas de recuperación de información, con sus diversas vertientes y la integración de tecnologías como el NLP y los modelos de tópicos, se perfilan como áreas de investigación y desarrollo de gran relevancia, subrayando la necesidad de enfoques especializados para manejar la complejidad y la riqueza del mundo de la información digital.

\section{Modelos de T\'opicos}

Los modelos de tópicos, fundamentales en el procesamiento del lenguaje natural y la recuperación de información, se destacan por su capacidad para explorar y organizar grandes conjuntos de datos textuales. Estos modelos operan identificando temas o "tópicos" subyacentes en colecciones de documentos, extrayendo patrones significativos en el uso de palabras y revelando así las estructuras temáticas latentes. Esta funcionalidad los convierte en herramientas esenciales para comprender, categorizar y sintetizar información en grandes volúmenes de texto, lo que es crucial en una era donde la cantidad de datos disponibles crece exponencialmente. Al aplicar estos modelos, es posible analizar tendencias y patrones en diversos campos, desde redes sociales hasta bibliotecas digitales, resaltando su versatilidad y valor para obtener insights y facilitar la toma de decisiones basada en datos. Su implementación ayuda a superar desafíos significativos relacionados con la sobrecarga de información, proporcionando un medio eficaz para filtrar y acceder a contenido relevante en un vasto mar de datos.

El desarrollo histórico de los modelos de tópicos es un viaje fascinante a través de la evolución en el procesamiento del lenguaje natural. Comenzando con el Análisis Semántico Latente (LSA) en 1990 por Deerwester et al. ("Indexing by latent semantic analysis"), un método pionero que utilizaba la descomposición de valor singular para identificar estructuras semánticas en grandes colecciones de texto. Esta técnica fue fundamental para sentar las bases de los modelos de tópicos, aunque carecía de un fundamento probabilístico.
El Análisis Semántico Latente Probabilístico (pLSA), propuesto por Hofmann en 1999, representó un avance significativo, introduciendo un enfoque probabilístico para modelar la relación entre documentos y tópicos. Sin embargo, pLSA tenía limitaciones, especialmente en la generalización a documentos no vistos.

El gran avance llegó con la introducción de la Asignación de Dirichlet Latente (LDA) por Blei et al. en 2003 ("Latent Dirichlet Allocation"), considerando cada documento como una mezcla de tópicos latentes, donde cada tópico está definido por una distribución sobre las palabras. Formalmente,
para cada documento \( d \), LDA asume una distribución de tópicos \( \theta_d \) que se extrae de una distribución a priori de Dirichlet. Para cada palabra en el documento, se elige un tópico \( z \) de la distribución de tópicos \( \theta_d \). Luego, se selecciona una palabra de una distribución de palabras asociada a ese tópico específico.
Este proceso se repite a lo largo de todos los documentos y palabras, iterando para ajustar las distribuciones de tópicos y palabras hasta que el modelo refleje adecuadamente la estructura latente de tópicos en los documentos. Este modelo generativo probabilístico ofreció una mayor flexibilidad y capacidad de interpretación, convirtiéndose en el estándar de oro para el modelado de tópicos.

Desde entonces, los modelos de tópicos han continuado evolucionando, integrando enfoques más sofisticados como el Modelo de Tópicos Correlacionados (CTM) y el Modelo de Tópicos Dinámicos (DTM). El CTM, al incorporar correlaciones entre tópicos, ofrece una representación más matizada y realista de cómo se distribuyen los tópicos en documentos, mientras que el DTM introduce una perspectiva temporal, analizando cómo los tópicos evolucionan con el tiempo. Además, los modelos jerárquicos como el Hierarchical Latent Dirichlet Allocation (HLDA) han proporcionado una estructura más compleja, permitiendo la detección de tópicos en distintos niveles de granularidad. Estos avances han enriquecido el análisis de tópicos, ofreciendo una visión más profunda y detallada de las estructuras temáticas en grandes volúmenes de texto.

%Los modelos de tópicos se aplican en una diversidad de campos, demostrando su versatilidad y utilidad. En plataformas de streaming y noticias, facilitan la recomendación de contenido relevante basado en los intereses del usuario. En redes sociales, son cruciales para analizar tendencias y discusiones. En la gestión de bibliotecas digitales, ayudan a organizar y buscar información temática. Son herramientas valiosas en la investigación académica para revisar y clasificar literatura científica. En marketing, identifican patrones de comportamiento y preferencias de los consumidores. En el sector financiero, se utilizan para análisis de sentimientos y predicción de tendencias de mercado. En salud pública, permiten analizar discursos y tendencias sobre temas de salud. En el ámbito gubernamental, facilitan la gestión y clasificación de documentos oficiales. En educación, son útiles para clasificar y recomendar material educativo, y en inteligencia de negocios, permiten el análisis de grandes volúmenes de datos y reportes.

Sin embargo, este campo no est\'a excento de retos. Determinar el número adecuado de tópicos es uno de los desafíos más significativos y tiene un impacto directo en la calidad y utilidad de los modelos ya que un número demasiado bajo puede fusionar tópicos distintos, mientras que un número excesivo puede resultar en tópicos superpuestos o irrelevantes, afectando la claridad y utilidad del análisis, tal como se detalla en el trabajo de Alghamdi y Alfalqi (2015). La interpretación de los tópicos generados también es un desafío crítico, ya que la subjetividad y variabilidad en la interpretación humana, como Chang et al. (2009) indican, pueden llevar a interpretaciones inconsistentes y dificultar la comparación entre estudios. Además, la gestión del ruido en los datos es esencial para asegurar que los tópicos reflejen temas genuinos. Estos desafíos resaltan la necesidad de herramientas y métodos más robustos para mejorar la interpretación y eficacia de los modelos de tópicos. 

\section{Ontolog\'ias}

Las ontologías, en el contexto de la informática y el procesamiento del lenguaje natural, son estructuras de datos que representan conocimientos de manera organizada y jerárquica. Según Guarino (1998) en "Formal Ontologies and Information Systems", una ontología define un conjunto de conceptos y categorías que representan un dominio de conocimiento, así como las relaciones entre estos conceptos. Computacionalmente, las ontologías se manejan frecuentemente como grafos, donde los nodos representan conceptos o entidades, y las aristas o bordes representan las relaciones entre ellos. Las ontologías permiten que las máquinas ``comprendan'' y procesen el significado de la información de manera más eficaz, fundamental para la extracción y clasificación de información, donde se requiere no solo identificar datos, sino también comprender su sem\'antica.

Según Uschold y Grüninger (1996), las ontologías son creadas principalmente por expertos en un dominio específico, con el objetivo de facilitar la comunicación y la comprensión común en diferentes campos. Estos expertos utilizan las ontologías para establecer un marco conceptual compartido, lo que ayuda a superar las barreras de comunicación y a mejorar la interoperabilidad entre sistemas y organizaciones, lo cual es fundamental en áreas como la ingeniería de sistemas, la integración empresarial y el desarrollo de software.

La construcción de ontologías es un proceso complejo que involucra varias etapas. Inicialmente, se realiza la identificación y definición de conceptos clave y sus interrelaciones en un dominio específico. Este proceso es generalmente llevado a cabo por expertos en el dominio en colaboración con ingenieros. Luego, se desarrolla una taxonomía que organiza estos conceptos de manera jerárquica. Además, se especifican propiedades y restricciones para estos conceptos y relaciones. Las ontologías se construyen utilizando lenguajes formales como OWL, permitiendo su uso en sistemas informáticos para mejorar la búsqueda y recuperación de información, facilitar la interoperabilidad entre sistemas, y proporcionar una base sólida para el desarrollo de aplicaciones de inteligencia artificial y la Web Semántica 

Las ontologías tienen aplicaciones diversas en la informática y el procesamiento del lenguaje natural. Berners-Lee et al. (2001) resaltan su uso en la Web Semántica, al mejorar la accesibilidad y gestión de información, facilitando búsquedas más eficientes y una navegación intuitiva. Hotho et al. (2002) y Lastra-Díaz et al. (2019) exploran su uso en la agrupación de documentos y la similitud semántica. Estas técnicas mejoran la precisión en la categorización y búsqueda de documentos, permitiendo a los sistemas informáticos identificar conexiones y similitudes en el contenido textual basándose en significados subyacentes. Batet et al. (2009) y Corcho (2006) se centran en la clasificación y anotación de documentos, demostrando cómo las ontologías enriquecen la recuperación y gestión de información. Su integración proporciona una mayor profundidad en el análisis de textos, permitiendo la creación de metadatos más ricos y relevantes, lo que mejora sustancialmente la recuperación y gestión de información en bases de datos y repositorios digitales.
Estos estudios ilustran la relevancia de las ontologías en campos variados, resaltando su papel esencial en la organización y comprensión de grandes volúmenes de datos.

\section{Vectores con contenido sem\'antico: \textit{embeddings}}

Los embeddings, en el contexto de procesamiento de lenguaje natural, son representaciones vectoriales de palabras o frases. Almeida y Xexéo (2023) en su estudio "Word Embeddings: A Survey", explican que estos embeddings son vectores densos y distribuidos de longitud fija, construidos utilizando estadísticas de co-ocurrencia de palabras. Estas representaciones codifican información sintáctica y semántica, lo que las hace útiles en una variedad de tareas de procesamiento del lenguaje natural. Los embeddings transforman el texto en una forma que es manejable para los algoritmos de aprendizaje automático, facilitando tareas como clasificación de texto, análisis de sentimientos y traducción automática.

Existen varios tipos de embeddings, cada uno con características únicas. Los word embeddings representan palabras individuales como vectores en un espacio multidimensional, capturando su significado y relaciones sintácticas. Los sentence embeddings, por otro lado, representan oraciones enteras, permitiendo capturar el contexto más amplio de la oración. Los contextual embeddings van un paso más allá, representando palabras en el contexto de frases o párrafos, lo que permite una comprensión más matizada del significado, especialmente para palabras con múltiples interpretaciones.

Los embeddings en procesamiento del lenguaje natural varían en su enfoque y aplicación. Los word embeddings, como Word2Vec y GloVe, representan palabras individuales en vectores densos, capturando relaciones semánticas y sintácticas. Word2Vec utiliza contextos locales de palabras, mientras que GloVe se basa en estadísticas globales de co-ocurrencia. Los sentence embeddings, como los generados por modelos como BERT, van más allá al representar oraciones enteras, capturando contextos más amplios y matices semánticos. BERT, en particular, utiliza embeddings contextuales, generando representaciones de palabras que varían según su contexto en una oración, lo que permite una comprensión más profunda y matizada del lenguaje. Estos modelos son fundamentales en tareas como análisis de sentimientos, traducción automática, y clasificación de texto, demostrando la diversidad y la profundidad del campo de los embeddings en procesamiento del lenguaje natural.

El estudio de Fang et al. (2016) demuestra la aplicación de embeddings en la evaluación de la coherencia de temas en datos de Twitter. Utilizando embeddings, se pueden medir con precisión la coherencia y relevancia de los tópicos generados, lo que es crucial para la interpretación efectiva de grandes conjuntos de datos sociales. Además, Kuzi et al. (2016) exploran cómo los embeddings pueden mejorar la expansión de consultas en motores de búsqueda, proporcionando una búsqueda más rica y contextualizada. Otros ejemplos incluyen el trabajo de Kusner et al., que examina cómo los embeddings de palabras se utilizan para medir distancias entre documentos, y Lezama-Sánchez et al. (2022), que investiga el uso de embeddings basados en relaciones semánticas para mejorar el análisis de texto. Además, Liu et al. exploran los embeddings temáticos, mostrando su aplicación en la comprensión de tópicos específicos en grandes conjuntos de datos. Estos ejemplos ilustran la versatilidad de los embeddings en diversas áreas para el análisis avanzado de textos y relaciones semánticas.

\section{Estado del arte}

La sección del estado del arte explora desarrollos recientes y metodologías clave en los dos problemas centrales a abordar en este trabajo: la automatizaci\'on de la identificación del número de tópicos en un corpus y de la asignaci\'on de nombres a t\'opicos. 

\subsection{Estimaci\'on del n\'umero de t\'opicos presentes en un corpus}

El enfoque de Arun et al. (2010) en "On Finding the Natural Number of Topics with Latent Dirichlet Allocation" se centra en determinar el número óptimo de tópicos para modelos LDA. Proponen una medida basada en la divergencia simétrica de Kullback-Leibler de las distribuciones salientes de los factores de la matriz LDA. Esta medida identifica el número de tópicos observando un 'pico' en los valores de divergencia para el número correcto de tópicos. El método es validado con conjuntos de datos reales y sintéticos, y destaca por su capacidad de discernir el número adecuado de tópicos, evitando tanto la sobre como la sub-representación de temas.

El enfoque de Jiang et al. (2011) en "A Fuzzy Self-Constructing Feature Clustering Algorithm for Text Classification" se basa en un algoritmo de agrupación de características autoconstruido difuso. Este método se enfoca en identificar estructuras latentes en datos de texto, utilizando técnicas de agrupación difusa para manejar la incertidumbre y la ambigüedad inherentes a los datos textuales. Su enfoque permite una clasificación más flexible y adaptativa de los datos, lo que puede ser especialmente útil en aplicaciones donde las categorías no son claramente definidas o donde los datos pueden pertenecer a múltiples categorías simultáneamente.

El enfoque de Thompson y Mimno (2020) en "Topic Modeling with Contextualized Word Representation Clusters" investiga el uso de agrupaciones de representaciones de palabras contextualizadas, como las de BERT y GPT-2, para el modelado de tópicos. Su metodología se basa en la hipótesis de que estas representaciones contextualizadas pueden capturar polisemia y proporcionar información sintáctica más rica, lo que resulta en una organización de documentos similar a la obtenida con modelos LDA tradicionales. Evalúan su enfoque con métricas como entropía de palabra y coherencia, encontrando que las incrustaciones contextualizadas pueden ser más consistentes sintácticamente que los tópicos LDA.

El enfoque de Gan y Qi (2021) en "Selection of the Optimal Number of Topics for LDA" se centra en una metodología integral para determinar el número óptimo de tópicos en LDA. Presentan un índice que evalúa varios factores: perplejidad, aislamiento de tópicos, estabilidad y coincidencia. Este índice tiene como objetivo lograr una alta capacidad predictiva, un buen aislamiento entre tópicos, evitar tópicos duplicados y asegurar la repetibilidad. Se validó con datasets generales y una aplicación específica en la clasificación de políticas de patentes en China, mostrando resultados de selección superiores a métodos existentes.

El enfoque de Vangara et al. (2021) en "Finding the Number of Latent Topics With Semantic Non-Negative Matrix Factorization" introduce SeNMFk, una metodología que combina la factorización de matrices no negativas (NMF) con información semántica para determinar el número óptimo de tópicos. SeNMFk utiliza la divergencia de Kullback-Leibler y se enfoca en la estabilidad de los tópicos a través de un ensamble aleatorio de matrices. Este enfoque mejora la identificación de tópicos y se valida comparándolo con técnicas existentes, demostrando su eficacia. Además, presentan el software pyDNMFk para facilitar la estimación del número de tópicos.

\subsection{Asignaci\'on de nombres a grupos}

El enfoque de "Ontologías y Etiquetado Semántico", como se explora en el trabajo de Boufaden (2003), utiliza ontologías para mejorar la asignación de etiquetas semánticas en sistemas de extracción de información. Las ontologías, que son representaciones estructuradas del conocimiento en un dominio específico, permiten identificar y clasificar información de manera más precisa y contextual. Al aplicar ontologías, el sistema puede comprender mejor la relación entre diferentes conceptos y términos, facilitando la asignación automática de nombres a tópicos. Este enfoque es particularmente útil en entornos donde es crítico comprender y categorizar correctamente la información, como en la extracción de información relevante de grandes conjuntos de datos o textos.

El enfoque de "Identificación de Tópicos en Páginas Web" utiliza técnicas de ontología para identificar y clasificar tópicos en páginas web. Este método, detallado en el trabajo de Singh Rathore y Roy (2014), implica el uso de ontologías para analizar y comprender el contenido de una página web, identificando temas y subtemas relevantes. La ontología, como un marco estructurado de conocimiento, permite una clasificación más precisa y contextual de la información, lo que facilita la asignación de etiquetas o nombres a los tópicos identificados. Este enfoque es particularmente útil para organizar y categorizar de manera eficiente la gran cantidad de información disponible en la web.

El enfoque que utiliza WordNet y TF-IDF para asignar nombres automáticamente a tópicos, como se describe en el trabajo de Saqlain et al. (2016), combina técnicas semánticas y estadísticas. WordNet se utiliza para entender la semántica de las palabras dentro de un clúster de texto, identificando hiperónimos (palabras de categoría superior) que pueden representar el tema general del clúster. Por otro lado, TF-IDF (Frecuencia de Término - Frecuencia Inversa de Documento) se emplea para identificar las palabras más significativas dentro del clúster. Al combinar ambos, se logra una etiqueta representativa que refleja tanto la importancia estadística de los términos como su relevancia semántica dentro del contexto del clúster. Este método es efectivo para capturar la esencia de un tópico y asignarle un nombre adecuado de manera automatizada.

\subsubsection{Desambiguaci\'on del sentido de las palabras}

La desambiguación de sentidos de palabras (WSD) es un campo de la lingüística computacional y el procesamiento del lenguaje natural que se enfoca en asignar significados precisos a palabras en contextos específicos, diferenciando entre múltiples significados o "sentidos". Dos documentos claves en este campo son "Word sense disambiguation using hybrid swarm intelligence approach" de AL-Saiagh et al. (2018) y "Word Sense Disambiguation—Algorithms and Applications" de Edmonds y Agirre (2008).

El documento de AL-Saiagh et al. (2018) introduce un enfoque híbrido que combina la optimización de enjambres de partículas (PSO) con el recocido simulado para WSD, utilizando medidas semánticas como JCN y Lesk extendido. 
Este estudio utiliza una versión extendida del algoritmo de Lesk, combinada con el método JCN (Jiang-Conrath), para realizar la desambiguación de sentidos de palabras (WSD). El algoritmo de Lesk, que se basa en el solapamiento de glosas (definiciones) de las palabras, se utiliza para medir la relación semántica entre diferentes sentidos de una palabra en un contexto dado. En este trabajo, la versión extendida de Lesk se emplea para encontrar superposiciones entre las glosas de los sentidos de las palabras y, en combinación con el método JCN, mejora la precisión en la tarea de WSD.

Por otro lado, Edmonds y Agirre (2008) proporcionan un análisis exhaustivo de los algoritmos y aplicaciones en WSD, abarcando desde enfoques supervisados que utilizan un conjunto cerrado de sentidos hasta métodos no supervisados que inducen sentidos directamente de los corpus. Este trabajo subraya la importancia de la optimización de parámetros y la evaluación comparativa de diferentes sistemas de WSD, ofreciendo una perspectiva integral sobre el estado del arte en este campo.

El documento de Bevilacqua et al. (2021) sobre tendencias recientes en la desambiguación de sentidos de palabras (WSD) explora enfoques y algoritmos avanzados en este campo. Se destacan los métodos basados en conocimiento, que utilizan algoritmos de grafos en redes semánticas, y los enfoques supervisados, que emplean arquitecturas neuronales y modelos preentrenados como BERT. Además, menciona modelos híbridos que combinan métodos supervisados y basados en conocimiento, el uso de definiciones (glosses) para clarificar sentidos, y la integración de modelos de lenguaje preentrenados, resaltando su impacto positivo en la precisión y generalización de la WSD.